{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning Methods Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning\n",
    "\n",
    "Group of predictors (classifiers / regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods:\n",
    "\n",
    "### **B**ootstrap **Agg**regat**ing** or [Bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n",
    "* [Scikit- Learn Reference](http://scikit-learn.org/stable/modules/ensemble.html#bagging)\n",
    "* Bootstrap sampling: Sampling with replacement\n",
    "* Combine by averaging the output (regression)\n",
    "* Combine by voting (classification)\n",
    "* Can be applied to many classifiers which includes ANN, CART, etc.\n",
    "\n",
    "### [Pasting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "* Sampling without replacement\n",
    "\n",
    "### [Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
    "* Train weak classifiers \n",
    "* Add them to a final strong classifier by weighting. Weighting by accuracy (typically)\n",
    "* Once added, the data are reweighted\n",
    "  * Misclassified samples gain weight \n",
    "  * Correctly classified samples lose weight (Exception: Boost by majority and BrownBoost - decrease the weight of repeatedly misclassified examples). \n",
    "  * Algo are forced to learn more from misclassified samples\n",
    "  \n",
    "    \n",
    "### [Stacking](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)\n",
    "* Also known as Stacked generalization\n",
    "* [From Kaggle:](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/) Combine information from multiple predictive models to generate a new model. Often times the stacked model (also called 2nd-level model) will outperform each of the individual models due its smoothing nature and ability to highlight each base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different. \n",
    "* Training a learning algorithm to combine the predictions of several other learning algorithms. \n",
    "  * Step 1: Train learning algo\n",
    "  * Step 2: Combiner algo is trained using algo predictions from step 1.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Ensemble methods**\n",
    "\n",
    "* Work best with indepedent predictors\n",
    "\n",
    "* Best to utilise different algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
